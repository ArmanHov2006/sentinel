================================================================================
SENTINEL: 60 HOURS ACROSS 9 DAYS (copy everything below this line)
================================================================================

HOW TIME IS ALLOCATED

- LEARN = new concept or technology. You do these yourself. Gets extra time.
- BUILD = pattern you already know. Ask AI to scaffold, then review. Compressed time.
- MIXED = learning + familiar mechanics. Moderate time.


================================================================================
DAY 1 (7h) -- STREAMING: OPENAI PROVIDER [LEARN]
================================================================================

Why this gets a full day: SSE (Server-Sent Events) is a protocol you have not implemented before. Async generators, parsing streaming HTTP line by line, partial JSON are new patterns.

What you learn:
- How SSE works (text/event-stream format, data: lines, [DONE] sentinel)
- How httpx streams responses with client.stream("POST", ...) and async iteration over lines
- How Python async generators (async def stream() -> AsyncIterator[str]) work
- How OpenAI's streaming format looks (data: {"choices": [{"delta": {"content": "..."}}]})

What you build (yourself):
- OpenAIProvider.stream() in src/sentinel/providers/openai.py
  - POST to /chat/completions with "stream": true
  - Use async with client.stream(...) to get the response
  - Iterate over lines, skip empty lines and data: [DONE]
  - Parse each data: {...} line as JSON, extract choices[0].delta.content
  - Yield each content chunk as a string
- Manual test: small script that calls stream() and prints chunks as they arrive

Key reference: Look at how _do_completion() builds headers and payload -- stream() uses same setup but adds "stream": true and reads the response differently.

Pace: Take your time. Read OpenAI streaming docs. Print raw response lines to understand format before parsing.


================================================================================
DAY 2 (7h) -- STREAMING: ANTHROPIC + CHAT API WIRING [LEARN]
================================================================================

Why this gets a full day: Anthropic's SSE format is different. You learn a second streaming protocol and wire both into the API (StreamingResponse, normalizing formats).

What you learn:
- Anthropic SSE events: message_start, content_block_start, content_block_delta, message_delta, message_stop
- How to normalize two streaming formats into one output format
- How FastAPI StreamingResponse works with an async generator

What you build:

Part A -- Anthropic streaming (yourself, ~4h):
- AnthropicProvider.stream() in src/sentinel/providers/anthropic.py
  - POST to /messages with "stream": true
  - Parse SSE: only yield text from content_block_delta where delta.type == "text_delta"
  - Handle message_stop as end of stream

Part B -- Wire into chat API (yourself, ~3h):
- Replace fake_stream_response() block in src/sentinel/api/v1/chat.py (lines 149-159)
- Create async generator that: gets provider from router, calls provider.stream(request), wraps each chunk in OpenAI SSE format (data: {"choices": [{"delta": {"content": "chunk"}}]}\n\n), sends data: [DONE]\n\n at end
- Feed that generator into StreamingResponse


================================================================================
DAY 3 (7h) -- DOCKER DEEP LEARNING DAY [LEARN]
================================================================================

Why this is a full day: You want to walk away understanding Docker. Not just "it works" but WHY each line exists.

Study plan (learning, not coding):

Hour 1-2: Core concepts
- What a container is vs a VM (namespaces, cgroups)
- What an image is (layers, union filesystem)
- What a registry is (Docker Hub, pulling images)
- Practice: docker pull python:3.11-slim, docker images, docker inspect

Hour 2-3: Dockerfile deep dive -- YOUR Dockerfile line by line:
- FROM python:3.11-slim -- why slim? other bases? base image?
- WORKDIR /app -- what it sets and why
- COPY requirements.txt . then RUN pip install BEFORE COPY . . -- layer caching, order matters
- RUN adduser ... && USER appuser -- why non-root (security)
- EXPOSE -- documentation vs actual port binding
- HEALTHCHECK -- how Docker uses this for orchestration
- CMD vs ENTRYPOINT -- when to use which
- Practice: build image, run it, exec into it, look around

Hour 3-4: Volumes and networking
- Volumes (redis-data:/data in compose) -- persistence across restarts
- Bind mounts (for dev: mount local code into container)
- Docker networking: bridge, how sentinel-network lets containers talk by service name
- Practice: docker network ls, docker volume ls, docker exec redis redis-cli ping

Hour 5-6: docker-compose deep dive -- YOUR docker-compose.yml:
- services block -- each service is a container
- build: . vs image: redis:7-alpine -- build from Dockerfile vs pull
- depends_on with condition: service_healthy -- startup ordering
- healthcheck -- how compose knows service is ready
- environment -- env vars, ${OPENAI_API_KEY} syntax
- profiles: [dev] -- optional services
- networks and volumes blocks
- Practice: docker compose up -d, docker compose ps, docker compose logs sentinel, docker compose down

Hour 7: Hands-on
- Add ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY} to sentinel in docker-compose.yml
- Add prometheus and grafana services to docker-compose.yml (configs on Day 5; get structure right now)
- docker compose up, debug if needed
- Multi-stage build exercise: try a multi-stage Dockerfile (build + runtime stage) to understand the concept


================================================================================
DAY 4 (7h) -- PROMETHEUS METRICS EXPORT [LEARN]
================================================================================

Why this gets a full day: Prometheus has its own data model (counters, gauges, histograms, labels). Industry standard; interviewers ask about it.

What you learn:
- Prometheus data model: counters (only up), gauges (up/down), histograms (bucketed)
- Labels: sentinel_requests_total{provider="openai", status="200"}
- Scraping: Prometheus pulls from your endpoint; you do not push
- The prometheus_client Python library

What you build (yourself):

Hour 1-2: Learn concepts
- Read Prometheus docs on metric types
- Install prometheus_client, play in REPL
- Create a Counter, increment, call generate_latest() to see output format

Hour 3-5: Implement src/sentinel/core/prometheus.py
- Map in-memory metrics to Prometheus: Counter(sentinel_requests_total), Histogram(sentinel_request_duration_seconds, buckets=[0.01, 0.05, 0.1, 0.25, 0.5, 1.0, 2.5, 5.0]), Gauge(sentinel_active_requests), Counter(sentinel_cache_hits_total), Counter(sentinel_cache_misses_total), Counter(sentinel_pii_detections_total), Counter(sentinel_injection_detections_total), Gauge(sentinel_circuit_breaker_state, labelnames=["provider"]) -- 0=closed, 1=open, 2=half-open
- Dual-write: add Prometheus calls alongside existing metrics.increment() etc.

Hour 5-7: Endpoint and test
- Add GET /metrics/prometheus returning Response(generate_latest(), media_type="text/plain")
- curl localhost:8000/metrics/prometheus -- verify Prometheus format
- Verify existing /metrics JSON and dashboard still work


================================================================================
DAY 5 (7h) -- GRAFANA DASHBOARD + MONITORING STACK [MIXED]
================================================================================

What you learn:
- Prometheus scraping (prometheus.yml)
- Grafana provisioning (datasources + dashboards from YAML/JSON)
- Grafana panels and PromQL

What you build:

Hour 1-2: Prometheus config + docker-compose (yourself)
- monitoring/prometheus.yml: scrape_configs with job_name sentinel, scrape_interval 15s, targets ["sentinel:8000"], metrics_path /metrics/prometheus
- Add prometheus and grafana services to docker-compose (from Day 3)
- docker compose up, verify Prometheus at localhost:9090

Hour 3-4: Grafana provisioning (yourself)
- monitoring/grafana/provisioning/datasources/prometheus.yml -> http://prometheus:9090
- monitoring/grafana/provisioning/dashboards/dashboards.yml -> dashboard JSON path
- Verify Grafana at localhost:3000 with Prometheus data source

Hour 5-7: Dashboard (ask AI to generate JSON, you customize)
- monitoring/grafana/dashboards/sentinel.json
- Panels: request rate rate(sentinel_requests_total[1m]), latency histogram_quantile(0.95, rate(sentinel_request_duration_seconds_bucket[5m])), error rate, cache hit rate, PII events, circuit breaker state
- Customize, verify data, screenshot for README


================================================================================
DAY 6 (5h) -- COST TRACKING + AUDIT LOG [BUILD - COMPRESSED]
================================================================================

Why compressed: Same pattern as CacheService -- service class, Redis, wire into chat.py, endpoints. Ask AI to scaffold, you review.

Hour 1-2: Cost tracking (AI scaffolds)
- src/sentinel/services/cost.py: TokenCostCalculator, pricing dict, calculate_cost()
- ChatResponse in src/sentinel/domain/models.py: cost_usd: float | None = None
- Add cost counters to src/sentinel/core/metrics.py
- GET /v1/usage endpoint
- Wire cost into both providers after completion

Hour 3-4: Audit log (AI scaffolds)
- src/sentinel/services/audit.py: AuditService, Redis list with TTL
- GET /v1/audit and GET /v1/audit/{request_id}
- Wire into chat completion in src/sentinel/api/v1/chat.py

Hour 5: Review + test
- Read AI output, run tests, manual test of new endpoints


================================================================================
DAY 7 (7h) -- LOAD TESTING WITH LOCUST [LEARN]
================================================================================

Why full day: Load testing is a new skill. Stress testing, reading results, finding bottlenecks.

What you learn:
- Locust: Python-based, user behavior classes
- Writing scenarios (tasks, wait times)
- Reading results (RPS, percentiles, error rates)
- Mocking external deps for isolated testing
- Identifying bottlenecks

What you build (yourself):

Hour 1-2: Learn Locust
- Install locust, quickstart
- Trivial test against /health
- Run: locust -f locustfile.py --host=http://localhost:8000
- Explore web UI and charts

Hour 3-4: Mock provider
- loadtests/mock_provider.py: tiny FastAPI app, canned OpenAI-format response, ~50ms delay
- Configure Sentinel OPENAI_BASE_URL to point at mock (e.g. http://localhost:9999/v1)

Hour 5-6: loadtests/locustfile.py scenarios
- Health (baseline)
- Chat completion cache miss (unique messages)
- Chat completion cache hit (same message)
- Streaming
- Ramp: 10, 50, 100 concurrent users

Hour 7: Capture results
- Run scenarios, screenshot Locust results
- Note peak RPS, p50/p95/p99, error rate
- Save for README Day 9


================================================================================
DAY 8 (7h) -- INTEGRATION TESTS + PRODUCTION HARDENING [MIXED]
================================================================================

Hour 1-5: Integration tests (yourself)
- tests/test_integration.py
- Concept: full request flow through FastAPI TestClient with mocked providers
- Tests: full flow (rate limiter -> PII -> injection -> cache -> provider -> response), streaming valid SSE, cache hit on second request, rate limiter blocks after threshold, circuit breaker opens, PII redaction, audit entry created, cost in metrics
- Mock httpx.AsyncClient.post so no real API calls
- Target: 20+ test cases

Hour 5-7: Production hardening (AI, you review)
- timeout=httpx.Timeout(settings.request_timeout_seconds) on all provider clients
- Input validation: max messages, max content length, reject empty
- Standardize error response format
- Config validation at startup
- CORS origins from settings instead of *


================================================================================
DAY 9 (6h) -- DOCUMENTATION, DEMO, POLISH [BUILD - COMPRESSED]
================================================================================

Hour 1-3: README (AI drafts, you polish)
- Anthropic in provider list and diagram
- Sections: Cost Tracking, Audit Log, Monitoring (Grafana screenshot), Performance (Day 7 numbers), Architecture Decisions
- Quick Start: docker compose up with full stack

Hour 3-5: Demo + finalization
- scripts/demo.py: completion, streaming, PII trigger, cost, audit
- docker-compose: all API keys, all services
- ruff check . && ruff format --check .
- pytest -v -- all green

Hour 5-6: Final polish
- Clean TODOs, verify CI, tag release if desired


================================================================================
TIME SUMMARY
================================================================================

Day 1  Streaming OpenAI           LEARN  7h   7h
Day 2  Streaming Anthropic+API    LEARN  7h  14h
Day 3  Docker deep learning       LEARN  7h  21h
Day 4  Prometheus                 LEARN  7h  28h
Day 5  Grafana + stack            MIXED  7h  35h
Day 6  Cost + Audit               BUILD  5h  40h
Day 7  Locust load testing        LEARN  7h  47h
Day 8  Integration + hardening    MIXED  7h  54h
Day 9  Docs demo polish           BUILD  6h  60h

LEARN (you do it): 35h
MIXED (guided):    14h
BUILD (AI, review): 11h


================================================================================
TODO CHECKLIST
================================================================================

[ ] Day 1: OpenAIProvider.stream(), manual test script
[ ] Day 2: AnthropicProvider.stream(), chat.py wiring, StreamingResponse
[ ] Day 3: Docker concepts, Dockerfile/compose deep dive, add prometheus/grafana to compose, ANTHROPIC_API_KEY
[ ] Day 4: prometheus.py, /metrics/prometheus, dual-write metrics
[ ] Day 5: prometheus.yml, Grafana provisioning, sentinel.json dashboard
[ ] Day 6: cost.py, audit.py, /v1/usage, /v1/audit
[ ] Day 7: Locust scenarios, mock_provider.py, capture results
[ ] Day 8: test_integration.py (20+ cases), hardening (timeouts, validation, CORS)
[ ] Day 9: README, demo.py, ruff + pytest green, CI

================================================================================
END OF 9-DAY PLAN
================================================================================
